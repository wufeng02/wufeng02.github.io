<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=7662302B2EE2DF855D33DA34FD827164><meta name=google-site-verification content=Nj_dp8eZCY7mCHa7wHJz4MXpILpTh7vii2xAujBW5Zg><title>Feng Wu (吴锋) | Automatic Generation of Robot Facial Expressions with Preferences</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Automatic Generation of Robot Facial Expressions with Preferences"><meta name=citation_author content="Bing Tang"><meta name=citation_author content="Rongyun Cao"><meta name=citation_author content="Rongya Chen"><meta name=citation_author content="Xiaoping Chen"><meta name=citation_author content="Bei Hua"><meta name=citation_author content="Feng Wu"><meta name=citation_book_title content="2023 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_inbook_title content="2023 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_conference_title content="2023 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_conference content="2023 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_firstpage content=7606><meta name=citation_lastpage content=7613><meta name=citation_publication_date content=2023><meta name=citation_online_date content=2023><meta name=citation_date content=2023><meta name=citation_year content=2023><meta name=citation_doi content=10.1109/ICRA48891.2023.10160409><meta name=citation_pdf_url content=//wufeng02.github.io/doc/pdf/TCCCHWicra23.pdf><meta name=citation_abstract content="The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications."><meta name=bepress_citation_title content="Automatic Generation of Robot Facial Expressions with Preferences"><meta name=bepress_citation_author content="Bing Tang"><meta name=bepress_citation_author content="Rongyun Cao"><meta name=bepress_citation_author content="Rongya Chen"><meta name=bepress_citation_author content="Xiaoping Chen"><meta name=bepress_citation_author content="Bei Hua"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_book_title content="2023 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=bepress_citation_firstpage content=7606><meta name=bepress_citation_lastpage content=7613><meta name=bepress_citation_date content=2023><meta name=bepress_citation_online_date content=2023><meta name=bepress_citation_doi content=10.1109/ICRA48891.2023.10160409><meta name=bepress_citation_pdf_url content=//wufeng02.github.io/doc/pdf/TCCCHWicra23.pdf><meta name=bepress_citation_abstract_html_url content=//wufeng02.github.io/doc/htm/TCCCHWicra23.html><meta name=bepress_citation_abstract content="The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications."><meta name=eprints.source content=//wufeng02.github.io/doc/htm/TCCCHWicra23.html><meta name=eprints.title content="Automatic Generation of Robot Facial Expressions with Preferences"><meta name=eprints.creators_name content="Bing Tang"><meta name=eprints.creators_name content="Rongyun Cao"><meta name=eprints.creators_name content="Rongya Chen"><meta name=eprints.creators_name content="Xiaoping Chen"><meta name=eprints.creators_name content="Bei Hua"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.type content=conference_item><meta name=eprints.event_type content=conference><meta name=eprints.event_title content="2023 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=eprints.date content=2023><meta name=eprints.date_type content=published><meta name=eprints.document_url content=//wufeng02.github.io/doc/pdf/TCCCHWicra23.pdf><meta name=eprints.citation content="Bing Tang, Rongyun Cao, Rongya Chen, Xiaoping Chen, Bei Hua, Feng Wu. Automatic Generation of Robot Facial Expressions with Preferences. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 7606-7613, London, UK, May 2023."><meta name=eprints.abstract content="The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=//wufeng02.github.io/doc/htm/TCCCHWicra23.html><meta name=DC.title content="Automatic Generation of Robot Facial Expressions with Preferences"><meta name=DC.creator content="Bing Tang"><meta name=DC.creator content="Rongyun Cao"><meta name=DC.creator content="Rongya Chen"><meta name=DC.creator content="Xiaoping Chen"><meta name=DC.creator content="Bei Hua"><meta name=DC.creator content="Feng Wu"><meta name=DC.type content="Conference or Workshop Item"><meta name=DC.relation.ispartof content="2023 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=DC.citation.spage content=7606><meta name=DC.citation.epage content=7613><meta name=DC.issued content=2023><meta name=DC.date content=2023><meta name=DC.format content=application/pdf><meta name=DC.identifier content=//wufeng02.github.io/doc/pdf/TCCCHWicra23.pdf><meta name=DC.description content="The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications."><meta name=twitter:site content=//wufeng02.github.io/doc/htm/TCCCHWicra23.html><meta property=twitter:title content="Automatic Generation of Robot Facial Expressions with Preferences"><meta property=twitter:description content="The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications."><meta property=og:site_name content=//wufeng02.github.io/doc/htm/TCCCHWicra23.html><meta property=og:title content="Automatic Generation of Robot Facial Expressions with Preferences"><meta property=og:url content=//wufeng02.github.io/doc/pdf/TCCCHWicra23.pdf><meta property=og:description content="The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications."><link rel=canonical href=//wufeng02.github.io/doc/htm/TCCCHWicra23.html><link rel=alternate hreflang=x-default href=//wufeng02.github.io/doc/htm/TCCCHWicra23.html><link rel=alternate hreflang=en href="//wufeng02.github.io/doc/htm/TCCCHWicra23.html?lang=en"><link rel=alternate hreflang=zh href="//wufeng02.github.io/doc/htm/TCCCHWicra23.html?lang=zh"><link rel=search type=text/html href=//wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=//wufeng02.github.io/search.xml><link rel=icon href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=//wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=//wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=//wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=//wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_book_title itemprop=name>2023 IEEE International Conference on Robotics and Automation (ICRA)</span>, </span> Page <span itemprop=pagination>7606-7613</span>, <span class="citation_date citation_year" itemprop=datePublished>2023</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Automatic Generation of Robot Facial Expressions with Preferences </h1><h3 id=author class=citation_author><span itemprop=author> Bing Tang</span>, <span itemprop=author>Rongyun Cao</span>, <span itemprop=author>Rongya Chen</span>, <span itemprop=author>Xiaoping Chen</span>, <span itemprop=author>Bei Hua</span>, <span itemprop=author>Feng Wu </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications.</p> &raquo; <a href=//wufeng02.github.io/doc/pdf/TCCCHWicra23.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 3.3MB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Bing Tang</span>, <span class=cite_a>Rongyun Cao</span>, <span class=cite_a>Rongya Chen</span>, <span class=cite_a>Xiaoping Chen</span>, <span class=cite_a>Bei Hua</span>, <span class=cite_a>Feng Wu</span>. <span class=cite_t>Automatic Generation of Robot Facial Expressions with Preferences</span>. In <span class=cite_p>2023 IEEE International Conference on Robotics and Automation (ICRA)</span>, <span class=cite_s>pages 7606-7613</span>, <span class=cite_l>London, UK,</span> <span class=cite_m>May </span><span class=cite_y>2023</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=//wufeng02.github.io/doc/htm/TCCCHWicra23.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>TCCCHWicra23</span><span class=p>,</span>
 <span class=na>address</span> <span class=p>=</span> <span class=s><span class=p>{</span>London, UK<span class=p>}</span></span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Bing Tang and Rongyun Cao and Rongya Chen and Xiaoping Chen and Bei Hua and Feng Wu<span class=p>}</span></span><span class=p>,</span>
 <span class=na>booktitle</span> <span class=p>=</span> <span class=s><span class=p>{</span>2023 IEEE International Conference on Robotics and Automation (ICRA)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>doi</span> <span class=p>=</span> <span class=s><span class=p>{</span>10.1109/ICRA48891.2023.10160409<span class=p>}</span></span><span class=p>,</span>
 <span class=na>month</span> <span class=p>=</span> <span class=s><span class=p>{</span>May<span class=p>}</span></span><span class=p>,</span>
 <span class=na>pages</span> <span class=p>=</span> <span class=s><span class=p>{</span>7606-7613<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Automatic Generation of Robot Facial Expressions with Preferences<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2023<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Automatic+Generation+of+Robot+Facial+Expressions+with+Preferences" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Automatic+Generation+of+Robot+Facial+Expressions+with+Preferences" target=_blank>Crossref</a> &mdash; DOI: <a href=https://doi.org/10.1109/ICRA48891.2023.10160409 target=_blank class=citation_doi>10.1109/ICRA48891.2023.10160409</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=//wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=//wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=//wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>