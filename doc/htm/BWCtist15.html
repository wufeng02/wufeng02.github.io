<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=n/a><meta name=google-site-verification content=TlGo1hAtAmt_Rcaql8ze8OrWwroTFpl2bfmPzbysQkY><title>Feng Wu (吴锋) | Online Planning for Large Markov Decision Processes with Hierarchical Decomposition</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Online Planning for Large Markov Decision Processes with Hierarchical Decomposition"><meta name=citation_author content="Aijun Bai"><meta name=citation_author content="Feng Wu"><meta name=citation_author content="Xiaoping Chen"><meta name=citation_journal_title content="ACM Transactions on Intelligent Systems and Technology"><meta name=citation_journal_abbrev content="ACM TIST"><meta name=citation_volume content=6(4)><meta name=citation_issue content=45><meta name=citation_publication_date content=2015><meta name=citation_online_date content=2015><meta name=citation_date content=2015><meta name=citation_year content=2015><meta name=citation_doi content=10.1145/2717316><meta name=citation_pdf_url content=https://wufeng02.github.io/doc/pdf/BWCtist15.pdf><meta name=citation_abstract content="Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ''curse of dimensionality'' - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP --- a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain --- a common benchmark for MDPs --- to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains."><meta name=bepress_citation_title content="Online Planning for Large Markov Decision Processes with Hierarchical Decomposition"><meta name=bepress_citation_author content="Aijun Bai"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_author content="Xiaoping Chen"><meta name=bepress_citation_journal_title content="ACM Transactions on Intelligent Systems and Technology (ACM TIST)"><meta name=bepress_citation_volume content=6(4)><meta name=bepress_citation_issue content=45><meta name=bepress_citation_date content=2015><meta name=bepress_citation_online_date content=2015><meta name=bepress_citation_doi content=10.1145/2717316><meta name=bepress_citation_pdf_url content=https://wufeng02.github.io/doc/pdf/BWCtist15.pdf><meta name=bepress_citation_abstract_html_url content=https://wufeng02.github.io/doc/htm/BWCtist15.html><meta name=bepress_citation_abstract content="Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ''curse of dimensionality'' - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP --- a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain --- a common benchmark for MDPs --- to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains."><meta name=eprints.source content=https://wufeng02.github.io/doc/htm/BWCtist15.html><meta name=eprints.title content="Online Planning for Large Markov Decision Processes with Hierarchical Decomposition"><meta name=eprints.creators_name content="Aijun Bai"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.creators_name content="Xiaoping Chen"><meta name=eprints.type content=article><meta name=eprints.publication content="ACM Transactions on Intelligent Systems and Technology (ACM TIST)"><meta name=eprints.volume content=6(4)><meta name=eprints.issue content=45><meta name=eprints.date content=2015><meta name=eprints.date_type content=published><meta name=eprints.document_url content=https://wufeng02.github.io/doc/pdf/BWCtist15.pdf><meta name=eprints.citation content="Aijun Bai, Feng Wu, Xiaoping Chen. Online Planning for Large Markov Decision Processes with Hierarchical Decomposition. ACM Transactions on Intelligent Systems and Technology (ACM TIST), 6(4)45August 2015."><meta name=eprints.abstract content="Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ''curse of dimensionality'' - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP --- a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain --- a common benchmark for MDPs --- to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=https://wufeng02.github.io/doc/htm/BWCtist15.html><meta name=DC.title content="Online Planning for Large Markov Decision Processes with Hierarchical Decomposition"><meta name=DC.creator content="Aijun Bai"><meta name=DC.creator content="Feng Wu"><meta name=DC.creator content="Xiaoping Chen"><meta name=DC.type content=Article><meta name=DC.relation.ispartof content="ACM Transactions on Intelligent Systems and Technology (ACM TIST)"><meta name=DC.citation.volume content=6(4)><meta name=DC.citation.issue content=45><meta name=DC.issued content=2015><meta name=DC.date content=2015><meta name=DC.format content=application/pdf><meta name=DC.identifier content=https://wufeng02.github.io/doc/pdf/BWCtist15.pdf><meta name=DC.description content="Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ''curse of dimensionality'' - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP --- a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain --- a common benchmark for MDPs --- to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains."><meta name=twitter:site content=https://wufeng02.github.io/doc/htm/BWCtist15.html><meta property=twitter:title content="Online Planning for Large Markov Decision Processes with Hierarchical Decomposition"><meta property=twitter:description content="Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ''curse of dimensionality'' - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP --- a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain --- a common benchmark for MDPs --- to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains."><meta property=og:site_name content=https://wufeng02.github.io/doc/htm/BWCtist15.html><meta property=og:title content="Online Planning for Large Markov Decision Processes with Hierarchical Decomposition"><meta property=og:url content=https://wufeng02.github.io/doc/pdf/BWCtist15.pdf><meta property=og:description content="Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ''curse of dimensionality'' - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP --- a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain --- a common benchmark for MDPs --- to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains."><link rel=canonical href=https://wufeng02.github.io/doc/htm/BWCtist15.html><link rel=alternate hreflang=x-default href=https://wufeng02.github.io/doc/htm/BWCtist15.html><link rel=alternate hreflang=en href="https://wufeng02.github.io/doc/htm/BWCtist15.html?lang=en"><link rel=alternate hreflang=zh href="https://wufeng02.github.io/doc/htm/BWCtist15.html?lang=zh"><link rel=search type=text/html href=https://wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=https://wufeng02.github.io/search.xml><link rel=icon href=https://wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=https://wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=https://wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=https://wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=https://wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=https://wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=https://wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=https://wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_journal_title itemprop=name>ACM Transactions on Intelligent Systems and Technology (ACM TIST)</span>, </span><span itemprop=isPartOf itemscope itemtype=http://schema.org/PublicationVolume> Volume <span class=citation_volume itemprop=volumeNumber>6(4)</span>, </span><span itemprop=isPartOf itemscope itemtype=http://schema.org/PublicationIssue> Number <span class=citation_issue itemprop=issueNumber>45</span>, </span><span class="citation_date citation_year" itemprop=datePublished>2015</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Online Planning for Large Markov Decision Processes with Hierarchical Decomposition </h1><h3 id=author class=citation_author><span itemprop=author> Aijun Bai</span>, <span itemprop=author>Feng Wu</span>, <span itemprop=author>Xiaoping Chen </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>Markov decision processes (MDPs) provide a rich framework for planning under uncertainty. However, exactly solving a large MDP is usually intractable due to the ''curse of dimensionality'' - the state space grows exponentially with the number of state variables. Online algorithms tackle this problem by avoiding computing a policy for the entire state space. On the other hand, since online algorithm has to find a near-optimal action online in almost real time, the computation time is often very limited. In the context of reinforcement learning, MAXQ is a value function decomposition method that exploits the underlying structure of the original MDP and decomposes it into a combination of smaller subproblems arranged over a task hierarchy. In this article, we present MAXQ-OP --- a novel online planning algorithm for large MDPs that utilizes MAXQ hierarchical decomposition in online settings. Compared to traditional online planning algorithms, MAXQ-OP is able to reach much more deeper states in the search tree with relatively less computation time by exploiting MAXQ hierarchical decomposition online. We empirically evaluate our algorithm in the standard Taxi domain --- a common benchmark for MDPs --- to show the effectiveness of our approach. We have also conducted a long-term case study in a highly complex simulated soccer domain and developed a team named WrightEagle that has won five world champions and five runners-up in the recent 10 years of RoboCup Soccer Simulation 2D annual competitions. The results in the RoboCup domain confirm the scalability of MAXQ-OP to very large domains.</p> &raquo; <a href=https://wufeng02.github.io/doc/pdf/BWCtist15.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 949.8KB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Aijun Bai</span>, <span class=cite_a>Feng Wu</span>, <span class=cite_a>Xiaoping Chen</span>. <span class=cite_t>Online Planning for Large Markov Decision Processes with Hierarchical Decomposition</span>. <span class=cite_p>ACM Transactions on Intelligent Systems and Technology (ACM TIST)</span>, <span class=cite_v>6(4)</span>(<span class=cite_n>45</span>)<span class=cite_m>August </span><span class=cite_y>2015</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=https://wufeng02.github.io/doc/htm/BWCtist15.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@article</span><span class=p>{</span><span class=nl>BWCtist15</span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Aijun Bai and Feng Wu and Xiaoping Chen<span class=p>}</span></span><span class=p>,</span>
 <span class=na>doi</span> <span class=p>=</span> <span class=s><span class=p>{</span>10.1145/2717316<span class=p>}</span></span><span class=p>,</span>
 <span class=na>journal</span> <span class=p>=</span> <span class=s><span class=p>{</span>ACM Transactions on Intelligent Systems and Technology (ACM TIST)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>month</span> <span class=p>=</span> <span class=s><span class=p>{</span>August<span class=p>}</span></span><span class=p>,</span>
 <span class=na>number</span> <span class=p>=</span> <span class=s><span class=p>{</span>45<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Online Planning for Large Markov Decision Processes with Hierarchical Decomposition<span class=p>}</span></span><span class=p>,</span>
 <span class=na>volume</span> <span class=p>=</span> <span class=s><span class=p>{</span>6(4)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2015<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Online+Planning+for+Large+Markov+Decision+Processes+with+Hierarchical+Decomposition" target=_blank>Google Scholar</a> &mdash; Cited by <a href="https://scholar.google.com/scholar?oi=bibs&hl=ja&cites=17238589413228693556" target=_blank>31</a></li><li><a href="https://search.crossref.org/?q=Online+Planning+for+Large+Markov+Decision+Processes+with+Hierarchical+Decomposition" target=_blank>Crossref</a> &mdash; DOI: <a href=https://doi.org/10.1145/2717316 target=_blank class=citation_doi>10.1145/2717316</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a> &mdash; Accession Number: 20153101079597 </li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a> &mdash; Accession Number: WOS:000360007500003 </li></ul></div><br></div><footer class=text-center><script type=text/javascript src=https://wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=https://wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=https://wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>