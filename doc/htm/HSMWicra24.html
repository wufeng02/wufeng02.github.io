<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=n/a><meta name=google-site-verification content=TlGo1hAtAmt_Rcaql8ze8OrWwroTFpl2bfmPzbysQkY><title>Feng Wu (吴锋) | Improving Offline Reinforcement Learning with Inaccurate Simulators</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Improving Offline Reinforcement Learning with Inaccurate Simulators"><meta name=citation_author content="Yiwen Hou"><meta name=citation_author content="Haoyuan Sun"><meta name=citation_author content="Jinming Ma"><meta name=citation_author content="Feng Wu"><meta name=citation_book_title content="2024 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_inbook_title content="2024 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_conference_title content="2024 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_conference content="2024 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=citation_firstpage content=5162><meta name=citation_lastpage content=5168><meta name=citation_publication_date content=2024><meta name=citation_online_date content=2024><meta name=citation_date content=2024><meta name=citation_year content=2024><meta name=citation_pdf_url content=//wufeng02.github.io/doc/pdf/HSMWicra24.pdf><meta name=citation_abstract content="Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."><meta name=bepress_citation_title content="Improving Offline Reinforcement Learning with Inaccurate Simulators"><meta name=bepress_citation_author content="Yiwen Hou"><meta name=bepress_citation_author content="Haoyuan Sun"><meta name=bepress_citation_author content="Jinming Ma"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_book_title content="2024 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=bepress_citation_firstpage content=5162><meta name=bepress_citation_lastpage content=5168><meta name=bepress_citation_date content=2024><meta name=bepress_citation_online_date content=2024><meta name=bepress_citation_pdf_url content=//wufeng02.github.io/doc/pdf/HSMWicra24.pdf><meta name=bepress_citation_abstract_html_url content=//wufeng02.github.io/doc/htm/HSMWicra24.html><meta name=bepress_citation_abstract content="Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."><meta name=eprints.source content=//wufeng02.github.io/doc/htm/HSMWicra24.html><meta name=eprints.title content="Improving Offline Reinforcement Learning with Inaccurate Simulators"><meta name=eprints.creators_name content="Yiwen Hou"><meta name=eprints.creators_name content="Haoyuan Sun"><meta name=eprints.creators_name content="Jinming Ma"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.type content=conference_item><meta name=eprints.event_type content=conference><meta name=eprints.event_title content="2024 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=eprints.date content=2024><meta name=eprints.date_type content=published><meta name=eprints.document_url content=//wufeng02.github.io/doc/pdf/HSMWicra24.pdf><meta name=eprints.citation content="Yiwen Hou, Haoyuan Sun, Jinming Ma, Feng Wu. Improving Offline Reinforcement Learning with Inaccurate Simulators. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 5162-5168, 2024."><meta name=eprints.abstract content="Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=//wufeng02.github.io/doc/htm/HSMWicra24.html><meta name=DC.title content="Improving Offline Reinforcement Learning with Inaccurate Simulators"><meta name=DC.creator content="Yiwen Hou"><meta name=DC.creator content="Haoyuan Sun"><meta name=DC.creator content="Jinming Ma"><meta name=DC.creator content="Feng Wu"><meta name=DC.type content="Conference or Workshop Item"><meta name=DC.relation.ispartof content="2024 IEEE International Conference on Robotics and Automation (ICRA)"><meta name=DC.citation.spage content=5162><meta name=DC.citation.epage content=5168><meta name=DC.issued content=2024><meta name=DC.date content=2024><meta name=DC.format content=application/pdf><meta name=DC.identifier content=//wufeng02.github.io/doc/pdf/HSMWicra24.pdf><meta name=DC.description content="Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."><meta name=twitter:site content=//wufeng02.github.io/doc/htm/HSMWicra24.html><meta property=twitter:title content="Improving Offline Reinforcement Learning with Inaccurate Simulators"><meta property=twitter:description content="Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."><meta property=og:site_name content=//wufeng02.github.io/doc/htm/HSMWicra24.html><meta property=og:title content="Improving Offline Reinforcement Learning with Inaccurate Simulators"><meta property=og:url content=//wufeng02.github.io/doc/pdf/HSMWicra24.pdf><meta property=og:description content="Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."><link rel=canonical href=//wufeng02.github.io/doc/htm/HSMWicra24.html><link rel=alternate hreflang=x-default href=//wufeng02.github.io/doc/htm/HSMWicra24.html><link rel=alternate hreflang=en href="//wufeng02.github.io/doc/htm/HSMWicra24.html?lang=en"><link rel=alternate hreflang=zh href="//wufeng02.github.io/doc/htm/HSMWicra24.html?lang=zh"><link rel=search type=text/html href=//wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=//wufeng02.github.io/search.xml><link rel=icon href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=//wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=//wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=//wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=//wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_book_title itemprop=name>2024 IEEE International Conference on Robotics and Automation (ICRA)</span>, </span> Page <span itemprop=pagination>5162-5168</span>, <span class="citation_date citation_year" itemprop=datePublished>2024</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Improving Offline Reinforcement Learning with Inaccurate Simulators </h1><h3 id=author class=citation_author><span itemprop=author> Yiwen Hou</span>, <span itemprop=author>Haoyuan Sun</span>, <span itemprop=author>Jinming Ma</span>, <span itemprop=author>Feng Wu </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods.</p> &raquo; <a href=//wufeng02.github.io/doc/pdf/HSMWicra24.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 1.7MB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Yiwen Hou</span>, <span class=cite_a>Haoyuan Sun</span>, <span class=cite_a>Jinming Ma</span>, <span class=cite_a>Feng Wu</span>. <span class=cite_t>Improving Offline Reinforcement Learning with Inaccurate Simulators</span>. In <span class=cite_p>2024 IEEE International Conference on Robotics and Automation (ICRA)</span>, <span class=cite_s>pages 5162-5168</span>, <span class=cite_y>2024</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=//wufeng02.github.io/doc/htm/HSMWicra24.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>HSMWicra24</span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Yiwen Hou and Haoyuan Sun and Jinming Ma and Feng Wu<span class=p>}</span></span><span class=p>,</span>
 <span class=na>booktitle</span> <span class=p>=</span> <span class=s><span class=p>{</span>2024 IEEE International Conference on Robotics and Automation (ICRA)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>pages</span> <span class=p>=</span> <span class=s><span class=p>{</span>5162-5168<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Improving Offline Reinforcement Learning with Inaccurate Simulators<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2024<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Improving+Offline+Reinforcement+Learning+with+Inaccurate+Simulators" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Improving+Offline+Reinforcement+Learning+with+Inaccurate+Simulators" target=_blank>Crossref</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=//wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=//wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=//wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>