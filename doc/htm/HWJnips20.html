<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=7662302B2EE2DF855D33DA34FD827164><meta name=google-site-verification content=Nj_dp8eZCY7mCHa7wHJz4MXpILpTh7vii2xAujBW5Zg><title>Feng Wu (吴锋) | Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping"><meta name=citation_author content="Yujing Hu"><meta name=citation_author content="Weixun Wang"><meta name=citation_author content="Hangtian Jia"><meta name=citation_author content="Yixiang Wang"><meta name=citation_author content="Yingfeng Chen"><meta name=citation_author content="Jianye Hao"><meta name=citation_author content="Feng Wu"><meta name=citation_author content="Changjie Fan"><meta name=citation_book_title content="Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)"><meta name=citation_inbook_title content="Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)"><meta name=citation_conference_title content="The 34th Conference on Neural Information Processing Systems (NIPS)"><meta name=citation_conference content="The 34th Conference on Neural Information Processing Systems (NIPS)"><meta name=citation_publication_date content=2020><meta name=citation_online_date content=2020><meta name=citation_date content=2020><meta name=citation_year content=2020><meta name=citation_pdf_url content=//wufeng02.github.io/doc/pdf/HWJnips20.pdf><meta name=citation_abstract content="Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones."><meta name=bepress_citation_title content="Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping"><meta name=bepress_citation_author content="Yujing Hu"><meta name=bepress_citation_author content="Weixun Wang"><meta name=bepress_citation_author content="Hangtian Jia"><meta name=bepress_citation_author content="Yixiang Wang"><meta name=bepress_citation_author content="Yingfeng Chen"><meta name=bepress_citation_author content="Jianye Hao"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_author content="Changjie Fan"><meta name=bepress_citation_book_title content="Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)"><meta name=bepress_citation_date content=2020><meta name=bepress_citation_online_date content=2020><meta name=bepress_citation_pdf_url content=//wufeng02.github.io/doc/pdf/HWJnips20.pdf><meta name=bepress_citation_abstract_html_url content=//wufeng02.github.io/doc/htm/HWJnips20.html><meta name=bepress_citation_abstract content="Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones."><meta name=eprints.source content=//wufeng02.github.io/doc/htm/HWJnips20.html><meta name=eprints.title content="Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping"><meta name=eprints.creators_name content="Yujing Hu"><meta name=eprints.creators_name content="Weixun Wang"><meta name=eprints.creators_name content="Hangtian Jia"><meta name=eprints.creators_name content="Yixiang Wang"><meta name=eprints.creators_name content="Yingfeng Chen"><meta name=eprints.creators_name content="Jianye Hao"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.creators_name content="Changjie Fan"><meta name=eprints.type content=conference_item><meta name=eprints.event_type content=conference><meta name=eprints.event_title content="Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)"><meta name=eprints.date content=2020><meta name=eprints.date_type content=published><meta name=eprints.document_url content=//wufeng02.github.io/doc/pdf/HWJnips20.pdf><meta name=eprints.citation content="Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, Changjie Fan. Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping. In Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS), December 2020."><meta name=eprints.abstract content="Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=//wufeng02.github.io/doc/htm/HWJnips20.html><meta name=DC.title content="Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping"><meta name=DC.creator content="Yujing Hu"><meta name=DC.creator content="Weixun Wang"><meta name=DC.creator content="Hangtian Jia"><meta name=DC.creator content="Yixiang Wang"><meta name=DC.creator content="Yingfeng Chen"><meta name=DC.creator content="Jianye Hao"><meta name=DC.creator content="Feng Wu"><meta name=DC.creator content="Changjie Fan"><meta name=DC.type content="Conference or Workshop Item"><meta name=DC.relation.ispartof content="Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)"><meta name=DC.issued content=2020><meta name=DC.date content=2020><meta name=DC.format content=application/pdf><meta name=DC.identifier content=//wufeng02.github.io/doc/pdf/HWJnips20.pdf><meta name=DC.description content="Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones."><meta name=twitter:site content=//wufeng02.github.io/doc/htm/HWJnips20.html><meta property=twitter:title content="Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping"><meta property=twitter:description content="Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones."><meta property=og:site_name content=//wufeng02.github.io/doc/htm/HWJnips20.html><meta property=og:title content="Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping"><meta property=og:url content=//wufeng02.github.io/doc/pdf/HWJnips20.pdf><meta property=og:description content="Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones."><link rel=canonical href=//wufeng02.github.io/doc/htm/HWJnips20.html><link rel=alternate hreflang=x-default href=//wufeng02.github.io/doc/htm/HWJnips20.html><link rel=alternate hreflang=en href="//wufeng02.github.io/doc/htm/HWJnips20.html?lang=en"><link rel=alternate hreflang=zh href="//wufeng02.github.io/doc/htm/HWJnips20.html?lang=zh"><link rel=search type=text/html href=//wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=//wufeng02.github.io/search.xml><link rel=icon href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=//wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=//wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=//wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=//wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_book_title itemprop=name>Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)</span>, </span><span class="citation_date citation_year" itemprop=datePublished>2020</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping </h1><h3 id=author class=citation_author><span itemprop=author> Yujing Hu</span>, <span itemprop=author>Weixun Wang</span>, <span itemprop=author>Hangtian Jia</span>, <span itemprop=author>Yixiang Wang</span>, <span itemprop=author>Yingfeng Chen</span>, <span itemprop=author>Jianye Hao</span>, <span itemprop=author>Feng Wu</span>, <span itemprop=author>Changjie Fan </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones.</p> &raquo; <a href=//wufeng02.github.io/doc/pdf/HWJnips20.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 7.9MB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Yujing Hu</span>, <span class=cite_a>Weixun Wang</span>, <span class=cite_a>Hangtian Jia</span>, <span class=cite_a>Yixiang Wang</span>, <span class=cite_a>Yingfeng Chen</span>, <span class=cite_a>Jianye Hao</span>, <span class=cite_a>Feng Wu</span>, <span class=cite_a>Changjie Fan</span>. <span class=cite_t>Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping</span>. In <span class=cite_p>Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)</span>, <span class=cite_m>December </span><span class=cite_y>2020</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=//wufeng02.github.io/doc/htm/HWJnips20.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>HWJnips20</span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Yujing Hu and Weixun Wang and Hangtian Jia and Yixiang Wang and Yingfeng Chen and Jianye Hao and Feng Wu and Changjie Fan<span class=p>}</span></span><span class=p>,</span>
 <span class=na>booktitle</span> <span class=p>=</span> <span class=s><span class=p>{</span>Proceedings of the 34th Conference on Neural Information Processing Systems (NIPS)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>month</span> <span class=p>=</span> <span class=s><span class=p>{</span>December<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2020<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Learning+to+Utilize+Shaping+Rewards:+A+New+Approach+of+Reward+Shaping" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Learning+to+Utilize+Shaping+Rewards:+A+New+Approach+of+Reward+Shaping" target=_blank>Crossref</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=//wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=//wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=//wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>