<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=7662302B2EE2DF855D33DA34FD827164><meta name=google-site-verification content=Nj_dp8eZCY7mCHa7wHJz4MXpILpTh7vii2xAujBW5Zg><title>Feng Wu (吴锋) | Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning"><meta name=citation_author content="Jinming Ma"><meta name=citation_author content="Feng Wu"><meta name=citation_book_title content="2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"><meta name=citation_inbook_title content="2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"><meta name=citation_conference_title content="2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"><meta name=citation_conference content="2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"><meta name=citation_firstpage content=5567><meta name=citation_lastpage content=5573><meta name=citation_publication_date content=2023><meta name=citation_online_date content=2023><meta name=citation_date content=2023><meta name=citation_year content=2023><meta name=citation_pdf_url content=//wufeng02.github.io/doc/pdf/MWiros23.pdf><meta name=citation_abstract content="Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines."><meta name=bepress_citation_title content="Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning"><meta name=bepress_citation_author content="Jinming Ma"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_book_title content="2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"><meta name=bepress_citation_firstpage content=5567><meta name=bepress_citation_lastpage content=5573><meta name=bepress_citation_date content=2023><meta name=bepress_citation_online_date content=2023><meta name=bepress_citation_pdf_url content=//wufeng02.github.io/doc/pdf/MWiros23.pdf><meta name=bepress_citation_abstract_html_url content=//wufeng02.github.io/doc/htm/MWiros23.html><meta name=bepress_citation_abstract content="Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines."><meta name=eprints.source content=//wufeng02.github.io/doc/htm/MWiros23.html><meta name=eprints.title content="Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning"><meta name=eprints.creators_name content="Jinming Ma"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.type content=conference_item><meta name=eprints.event_type content=conference><meta name=eprints.event_title content="2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"><meta name=eprints.date content=2023><meta name=eprints.date_type content=published><meta name=eprints.document_url content=//wufeng02.github.io/doc/pdf/MWiros23.pdf><meta name=eprints.citation content="Jinming Ma, Feng Wu. Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5567-5573, Detroit, USA, October 2023."><meta name=eprints.abstract content="Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=//wufeng02.github.io/doc/htm/MWiros23.html><meta name=DC.title content="Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning"><meta name=DC.creator content="Jinming Ma"><meta name=DC.creator content="Feng Wu"><meta name=DC.type content="Conference or Workshop Item"><meta name=DC.relation.ispartof content="2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"><meta name=DC.citation.spage content=5567><meta name=DC.citation.epage content=5573><meta name=DC.issued content=2023><meta name=DC.date content=2023><meta name=DC.format content=application/pdf><meta name=DC.identifier content=//wufeng02.github.io/doc/pdf/MWiros23.pdf><meta name=DC.description content="Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines."><meta name=twitter:site content=//wufeng02.github.io/doc/htm/MWiros23.html><meta property=twitter:title content="Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning"><meta property=twitter:description content="Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines."><meta property=og:site_name content=//wufeng02.github.io/doc/htm/MWiros23.html><meta property=og:title content="Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning"><meta property=og:url content=//wufeng02.github.io/doc/pdf/MWiros23.pdf><meta property=og:description content="Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines."><link rel=canonical href=//wufeng02.github.io/doc/htm/MWiros23.html><link rel=alternate hreflang=x-default href=//wufeng02.github.io/doc/htm/MWiros23.html><link rel=alternate hreflang=en href="//wufeng02.github.io/doc/htm/MWiros23.html?lang=en"><link rel=alternate hreflang=zh href="//wufeng02.github.io/doc/htm/MWiros23.html?lang=zh"><link rel=search type=text/html href=//wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=//wufeng02.github.io/search.xml><link rel=icon href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=//wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=//wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=//wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=//wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_book_title itemprop=name>2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, </span> Page <span itemprop=pagination>5567-5573</span>, <span class="citation_date citation_year" itemprop=datePublished>2023</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning </h1><h3 id=author class=citation_author><span itemprop=author> Jinming Ma</span>, <span itemprop=author>Feng Wu </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines.</p> &raquo; <a href=//wufeng02.github.io/doc/pdf/MWiros23.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 4.0MB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Jinming Ma</span>, <span class=cite_a>Feng Wu</span>. <span class=cite_t>Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning</span>. In <span class=cite_p>2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, <span class=cite_s>pages 5567-5573</span>, <span class=cite_l>Detroit, USA,</span> <span class=cite_m>October </span><span class=cite_y>2023</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=//wufeng02.github.io/doc/htm/MWiros23.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>MWiros23</span><span class=p>,</span>
 <span class=na>address</span> <span class=p>=</span> <span class=s><span class=p>{</span>Detroit, USA<span class=p>}</span></span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Jinming Ma and Feng Wu<span class=p>}</span></span><span class=p>,</span>
 <span class=na>booktitle</span> <span class=p>=</span> <span class=s><span class=p>{</span>2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>month</span> <span class=p>=</span> <span class=s><span class=p>{</span>October<span class=p>}</span></span><span class=p>,</span>
 <span class=na>pages</span> <span class=p>=</span> <span class=s><span class=p>{</span>5567-5573<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2023<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Effective+Traffic+Signal+Control+with+Offline-to-Online+Reinforcement+Learning" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Effective+Traffic+Signal+Control+with+Offline-to-Online+Reinforcement+Learning" target=_blank>Crossref</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=//wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=//wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=//wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>