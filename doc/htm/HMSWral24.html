<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=n/a><meta name=google-site-verification content=TlGo1hAtAmt_Rcaql8ze8OrWwroTFpl2bfmPzbysQkY><title>Feng Wu (吴锋) | Effective Offline Robot Learning with Structured Task Graph</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Effective Offline Robot Learning with Structured Task Graph"><meta name=citation_author content="Yiwen Hou"><meta name=citation_author content="Jinming Ma"><meta name=citation_author content="Haoyuan Sun"><meta name=citation_author content="Feng Wu"><meta name=citation_journal_title content="IEEE Robotics and Automation Letters"><meta name=citation_journal_abbrev content=RAL><meta name=citation_firstpage content=1><meta name=citation_lastpage content=8><meta name=citation_publication_date content=2024><meta name=citation_online_date content=2024><meta name=citation_date content=2024><meta name=citation_year content=2024><meta name=citation_doi content=10.1109/LRA.2024.3354620><meta name=citation_pdf_url content=//wufeng02.github.io/doc/pdf/HMSWral24.pdf><meta name=citation_abstract content="Offline reinforcement learning (RL) has shown great potential in many robotic tasks, where doing trial-and-error with the environment is risky, costly, or time-consuming. However, it is still hard to succeed in long-horizon tasks especially when given suboptimal and multimodal offline datasets. Nevertheless, existing RL methods rarely consider the structured information in offline datasets, which are commonly found in many robotic tasks. To address these challenges, we propose a novel offline RL approach that combines the techniques of dataset augmentation and subtask relabeling. Specifically, we first extract the subtasks and build the task graph based on the structured information in offline datasets. We then use the task graph to sample and generate an augmented dataset, which is more suitable for offline RL learning. After that, we relabel the dataset according to the task graph and finally learn a subtask-conditioned policy to complete the task. By doing so, we decompose the task of reaching a long-horizon goal state into a sequence of easier subtasks. This is not only useful for handling the long-horizon problem, but also reduces the error introduced by the offline dataset. We conducted extensive experiments in both the D4RL benchmark dataset and real-world robot with complex manipulation tasks. The experimental results show that our method significantly advances the state-of-the-art baselines in most tasks, particularly in longhorizon manipulation tasks with limited human demonstrations."><meta name=bepress_citation_title content="Effective Offline Robot Learning with Structured Task Graph"><meta name=bepress_citation_author content="Yiwen Hou"><meta name=bepress_citation_author content="Jinming Ma"><meta name=bepress_citation_author content="Haoyuan Sun"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_journal_title content="IEEE Robotics and Automation Letters (RAL)"><meta name=bepress_citation_firstpage content=1><meta name=bepress_citation_lastpage content=8><meta name=bepress_citation_date content=2024><meta name=bepress_citation_online_date content=2024><meta name=bepress_citation_doi content=10.1109/LRA.2024.3354620><meta name=bepress_citation_pdf_url content=//wufeng02.github.io/doc/pdf/HMSWral24.pdf><meta name=bepress_citation_abstract_html_url content=//wufeng02.github.io/doc/htm/HMSWral24.html><meta name=bepress_citation_abstract content="Offline reinforcement learning (RL) has shown great potential in many robotic tasks, where doing trial-and-error with the environment is risky, costly, or time-consuming. However, it is still hard to succeed in long-horizon tasks especially when given suboptimal and multimodal offline datasets. Nevertheless, existing RL methods rarely consider the structured information in offline datasets, which are commonly found in many robotic tasks. To address these challenges, we propose a novel offline RL approach that combines the techniques of dataset augmentation and subtask relabeling. Specifically, we first extract the subtasks and build the task graph based on the structured information in offline datasets. We then use the task graph to sample and generate an augmented dataset, which is more suitable for offline RL learning. After that, we relabel the dataset according to the task graph and finally learn a subtask-conditioned policy to complete the task. By doing so, we decompose the task of reaching a long-horizon goal state into a sequence of easier subtasks. This is not only useful for handling the long-horizon problem, but also reduces the error introduced by the offline dataset. We conducted extensive experiments in both the D4RL benchmark dataset and real-world robot with complex manipulation tasks. The experimental results show that our method significantly advances the state-of-the-art baselines in most tasks, particularly in longhorizon manipulation tasks with limited human demonstrations."><meta name=eprints.source content=//wufeng02.github.io/doc/htm/HMSWral24.html><meta name=eprints.title content="Effective Offline Robot Learning with Structured Task Graph"><meta name=eprints.creators_name content="Yiwen Hou"><meta name=eprints.creators_name content="Jinming Ma"><meta name=eprints.creators_name content="Haoyuan Sun"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.type content=article><meta name=eprints.publication content="IEEE Robotics and Automation Letters (RAL)"><meta name=eprints.date content=2024><meta name=eprints.date_type content=published><meta name=eprints.document_url content=//wufeng02.github.io/doc/pdf/HMSWral24.pdf><meta name=eprints.citation content="Yiwen Hou, Jinming Ma, Haoyuan Sun, Feng Wu. Effective Offline Robot Learning with Structured Task Graph. IEEE Robotics and Automation Letters (RAL), :1-8, 2024."><meta name=eprints.abstract content="Offline reinforcement learning (RL) has shown great potential in many robotic tasks, where doing trial-and-error with the environment is risky, costly, or time-consuming. However, it is still hard to succeed in long-horizon tasks especially when given suboptimal and multimodal offline datasets. Nevertheless, existing RL methods rarely consider the structured information in offline datasets, which are commonly found in many robotic tasks. To address these challenges, we propose a novel offline RL approach that combines the techniques of dataset augmentation and subtask relabeling. Specifically, we first extract the subtasks and build the task graph based on the structured information in offline datasets. We then use the task graph to sample and generate an augmented dataset, which is more suitable for offline RL learning. After that, we relabel the dataset according to the task graph and finally learn a subtask-conditioned policy to complete the task. By doing so, we decompose the task of reaching a long-horizon goal state into a sequence of easier subtasks. This is not only useful for handling the long-horizon problem, but also reduces the error introduced by the offline dataset. We conducted extensive experiments in both the D4RL benchmark dataset and real-world robot with complex manipulation tasks. The experimental results show that our method significantly advances the state-of-the-art baselines in most tasks, particularly in longhorizon manipulation tasks with limited human demonstrations."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=//wufeng02.github.io/doc/htm/HMSWral24.html><meta name=DC.title content="Effective Offline Robot Learning with Structured Task Graph"><meta name=DC.creator content="Yiwen Hou"><meta name=DC.creator content="Jinming Ma"><meta name=DC.creator content="Haoyuan Sun"><meta name=DC.creator content="Feng Wu"><meta name=DC.type content=Article><meta name=DC.relation.ispartof content="IEEE Robotics and Automation Letters (RAL)"><meta name=DC.citation.spage content=1><meta name=DC.citation.epage content=8><meta name=DC.issued content=2024><meta name=DC.date content=2024><meta name=DC.format content=application/pdf><meta name=DC.identifier content=//wufeng02.github.io/doc/pdf/HMSWral24.pdf><meta name=DC.description content="Offline reinforcement learning (RL) has shown great potential in many robotic tasks, where doing trial-and-error with the environment is risky, costly, or time-consuming. However, it is still hard to succeed in long-horizon tasks especially when given suboptimal and multimodal offline datasets. Nevertheless, existing RL methods rarely consider the structured information in offline datasets, which are commonly found in many robotic tasks. To address these challenges, we propose a novel offline RL approach that combines the techniques of dataset augmentation and subtask relabeling. Specifically, we first extract the subtasks and build the task graph based on the structured information in offline datasets. We then use the task graph to sample and generate an augmented dataset, which is more suitable for offline RL learning. After that, we relabel the dataset according to the task graph and finally learn a subtask-conditioned policy to complete the task. By doing so, we decompose the task of reaching a long-horizon goal state into a sequence of easier subtasks. This is not only useful for handling the long-horizon problem, but also reduces the error introduced by the offline dataset. We conducted extensive experiments in both the D4RL benchmark dataset and real-world robot with complex manipulation tasks. The experimental results show that our method significantly advances the state-of-the-art baselines in most tasks, particularly in longhorizon manipulation tasks with limited human demonstrations."><meta name=twitter:site content=//wufeng02.github.io/doc/htm/HMSWral24.html><meta property=twitter:title content="Effective Offline Robot Learning with Structured Task Graph"><meta property=twitter:description content="Offline reinforcement learning (RL) has shown great potential in many robotic tasks, where doing trial-and-error with the environment is risky, costly, or time-consuming. However, it is still hard to succeed in long-horizon tasks especially when given suboptimal and multimodal offline datasets. Nevertheless, existing RL methods rarely consider the structured information in offline datasets, which are commonly found in many robotic tasks. To address these challenges, we propose a novel offline RL approach that combines the techniques of dataset augmentation and subtask relabeling. Specifically, we first extract the subtasks and build the task graph based on the structured information in offline datasets. We then use the task graph to sample and generate an augmented dataset, which is more suitable for offline RL learning. After that, we relabel the dataset according to the task graph and finally learn a subtask-conditioned policy to complete the task. By doing so, we decompose the task of reaching a long-horizon goal state into a sequence of easier subtasks. This is not only useful for handling the long-horizon problem, but also reduces the error introduced by the offline dataset. We conducted extensive experiments in both the D4RL benchmark dataset and real-world robot with complex manipulation tasks. The experimental results show that our method significantly advances the state-of-the-art baselines in most tasks, particularly in longhorizon manipulation tasks with limited human demonstrations."><meta property=og:site_name content=//wufeng02.github.io/doc/htm/HMSWral24.html><meta property=og:title content="Effective Offline Robot Learning with Structured Task Graph"><meta property=og:url content=//wufeng02.github.io/doc/pdf/HMSWral24.pdf><meta property=og:description content="Offline reinforcement learning (RL) has shown great potential in many robotic tasks, where doing trial-and-error with the environment is risky, costly, or time-consuming. However, it is still hard to succeed in long-horizon tasks especially when given suboptimal and multimodal offline datasets. Nevertheless, existing RL methods rarely consider the structured information in offline datasets, which are commonly found in many robotic tasks. To address these challenges, we propose a novel offline RL approach that combines the techniques of dataset augmentation and subtask relabeling. Specifically, we first extract the subtasks and build the task graph based on the structured information in offline datasets. We then use the task graph to sample and generate an augmented dataset, which is more suitable for offline RL learning. After that, we relabel the dataset according to the task graph and finally learn a subtask-conditioned policy to complete the task. By doing so, we decompose the task of reaching a long-horizon goal state into a sequence of easier subtasks. This is not only useful for handling the long-horizon problem, but also reduces the error introduced by the offline dataset. We conducted extensive experiments in both the D4RL benchmark dataset and real-world robot with complex manipulation tasks. The experimental results show that our method significantly advances the state-of-the-art baselines in most tasks, particularly in longhorizon manipulation tasks with limited human demonstrations."><link rel=canonical href=//wufeng02.github.io/doc/htm/HMSWral24.html><link rel=alternate hreflang=x-default href=//wufeng02.github.io/doc/htm/HMSWral24.html><link rel=alternate hreflang=en href="//wufeng02.github.io/doc/htm/HMSWral24.html?lang=en"><link rel=alternate hreflang=zh href="//wufeng02.github.io/doc/htm/HMSWral24.html?lang=zh"><link rel=search type=text/html href=//wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=//wufeng02.github.io/search.xml><link rel=icon href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=//wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=//wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=//wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=//wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_journal_title itemprop=name>IEEE Robotics and Automation Letters (RAL)</span>, </span> Page <span itemprop=pagination>1-8</span>, <span class="citation_date citation_year" itemprop=datePublished>2024</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Effective Offline Robot Learning with Structured Task Graph </h1><h3 id=author class=citation_author><span itemprop=author> Yiwen Hou</span>, <span itemprop=author>Jinming Ma</span>, <span itemprop=author>Haoyuan Sun</span>, <span itemprop=author>Feng Wu </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>Offline reinforcement learning (RL) has shown great potential in many robotic tasks, where doing trial-and-error with the environment is risky, costly, or time-consuming. However, it is still hard to succeed in long-horizon tasks especially when given suboptimal and multimodal offline datasets. Nevertheless, existing RL methods rarely consider the structured information in offline datasets, which are commonly found in many robotic tasks. To address these challenges, we propose a novel offline RL approach that combines the techniques of dataset augmentation and subtask relabeling. Specifically, we first extract the subtasks and build the task graph based on the structured information in offline datasets. We then use the task graph to sample and generate an augmented dataset, which is more suitable for offline RL learning. After that, we relabel the dataset according to the task graph and finally learn a subtask-conditioned policy to complete the task. By doing so, we decompose the task of reaching a long-horizon goal state into a sequence of easier subtasks. This is not only useful for handling the long-horizon problem, but also reduces the error introduced by the offline dataset. We conducted extensive experiments in both the D4RL benchmark dataset and real-world robot with complex manipulation tasks. The experimental results show that our method significantly advances the state-of-the-art baselines in most tasks, particularly in longhorizon manipulation tasks with limited human demonstrations.</p> &raquo; <a href=//wufeng02.github.io/doc/pdf/HMSWral24.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 1.6MB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Yiwen Hou</span>, <span class=cite_a>Jinming Ma</span>, <span class=cite_a>Haoyuan Sun</span>, <span class=cite_a>Feng Wu</span>. <span class=cite_t>Effective Offline Robot Learning with Structured Task Graph</span>. <span class=cite_p>IEEE Robotics and Automation Letters (RAL)</span>, :<span class=cite_s>1-8</span>, <span class=cite_y>2024</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=//wufeng02.github.io/doc/htm/HMSWral24.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@article</span><span class=p>{</span><span class=nl>HMSWral24</span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Yiwen Hou and Jinming Ma and Haoyuan Sun and Feng Wu<span class=p>}</span></span><span class=p>,</span>
 <span class=na>doi</span> <span class=p>=</span> <span class=s><span class=p>{</span>10.1109/LRA.2024.3354620<span class=p>}</span></span><span class=p>,</span>
 <span class=na>journal</span> <span class=p>=</span> <span class=s><span class=p>{</span>IEEE Robotics and Automation Letters (RAL)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>pages</span> <span class=p>=</span> <span class=s><span class=p>{</span>1-8<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Effective Offline Robot Learning with Structured Task Graph<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2024<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Effective+Offline+Robot+Learning+with+Structured+Task+Graph" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Effective+Offline+Robot+Learning+with+Structured+Task+Graph" target=_blank>Crossref</a> &mdash; DOI: <a href=https://doi.org/10.1109/LRA.2024.3354620 target=_blank class=citation_doi>10.1109/LRA.2024.3354620</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=//wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=//wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=//wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>