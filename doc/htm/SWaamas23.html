<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=n/a><meta name=google-site-verification content=TlGo1hAtAmt_Rcaql8ze8OrWwroTFpl2bfmPzbysQkY><title>Feng Wu (吴锋) | Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines"><meta name=citation_author content="Haoyuan Sun"><meta name=citation_author content="Feng Wu"><meta name=citation_book_title content="Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"><meta name=citation_inbook_title content="Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"><meta name=citation_conference_title content="The 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"><meta name=citation_conference content="The 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"><meta name=citation_firstpage content=1239><meta name=citation_lastpage content=1247><meta name=citation_publication_date content=2023><meta name=citation_online_date content=2023><meta name=citation_date content=2023><meta name=citation_year content=2023><meta name=citation_pdf_url content=https://wufeng02.github.io/doc/pdf/SWaamas23.pdf><meta name=citation_abstract content="Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets."><meta name=bepress_citation_title content="Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines"><meta name=bepress_citation_author content="Haoyuan Sun"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_book_title content="Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"><meta name=bepress_citation_firstpage content=1239><meta name=bepress_citation_lastpage content=1247><meta name=bepress_citation_date content=2023><meta name=bepress_citation_online_date content=2023><meta name=bepress_citation_pdf_url content=https://wufeng02.github.io/doc/pdf/SWaamas23.pdf><meta name=bepress_citation_abstract_html_url content=https://wufeng02.github.io/doc/htm/SWaamas23.html><meta name=bepress_citation_abstract content="Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets."><meta name=eprints.source content=https://wufeng02.github.io/doc/htm/SWaamas23.html><meta name=eprints.title content="Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines"><meta name=eprints.creators_name content="Haoyuan Sun"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.type content=conference_item><meta name=eprints.event_type content=conference><meta name=eprints.event_title content="Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"><meta name=eprints.date content=2023><meta name=eprints.date_type content=published><meta name=eprints.document_url content=https://wufeng02.github.io/doc/pdf/SWaamas23.pdf><meta name=eprints.citation content="Haoyuan Sun, Feng Wu. Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 1239-1247, London, UK, May 2023."><meta name=eprints.abstract content="Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=https://wufeng02.github.io/doc/htm/SWaamas23.html><meta name=DC.title content="Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines"><meta name=DC.creator content="Haoyuan Sun"><meta name=DC.creator content="Feng Wu"><meta name=DC.type content="Conference or Workshop Item"><meta name=DC.relation.ispartof content="Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)"><meta name=DC.citation.spage content=1239><meta name=DC.citation.epage content=1247><meta name=DC.issued content=2023><meta name=DC.date content=2023><meta name=DC.format content=application/pdf><meta name=DC.identifier content=https://wufeng02.github.io/doc/pdf/SWaamas23.pdf><meta name=DC.description content="Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets."><meta name=twitter:site content=https://wufeng02.github.io/doc/htm/SWaamas23.html><meta property=twitter:title content="Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines"><meta property=twitter:description content="Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets."><meta property=og:site_name content=https://wufeng02.github.io/doc/htm/SWaamas23.html><meta property=og:title content="Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines"><meta property=og:url content=https://wufeng02.github.io/doc/pdf/SWaamas23.pdf><meta property=og:description content="Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets."><link rel=canonical href=https://wufeng02.github.io/doc/htm/SWaamas23.html><link rel=alternate hreflang=x-default href=https://wufeng02.github.io/doc/htm/SWaamas23.html><link rel=alternate hreflang=en href="https://wufeng02.github.io/doc/htm/SWaamas23.html?lang=en"><link rel=alternate hreflang=zh href="https://wufeng02.github.io/doc/htm/SWaamas23.html?lang=zh"><link rel=search type=text/html href=https://wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=https://wufeng02.github.io/search.xml><link rel=icon href=https://wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=https://wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=https://wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=https://wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=https://wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=https://wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=https://wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=https://wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_book_title itemprop=name>Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</span>, </span> Page <span itemprop=pagination>1239-1247</span>, <span class="citation_date citation_year" itemprop=datePublished>2023</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines </h1><h3 id=author class=citation_author><span itemprop=author> Haoyuan Sun</span>, <span itemprop=author>Feng Wu </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets.</p> &raquo; <a href=https://wufeng02.github.io/doc/pdf/SWaamas23.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 3.7MB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Haoyuan Sun</span>, <span class=cite_a>Feng Wu</span>. <span class=cite_t>Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines</span>. In <span class=cite_p>Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</span>, <span class=cite_s>pages 1239-1247</span>, <span class=cite_l>London, UK,</span> <span class=cite_m>May </span><span class=cite_y>2023</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=https://wufeng02.github.io/doc/htm/SWaamas23.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>SWaamas23</span><span class=p>,</span>
 <span class=na>address</span> <span class=p>=</span> <span class=s><span class=p>{</span>London, UK<span class=p>}</span></span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Haoyuan Sun and Feng Wu<span class=p>}</span></span><span class=p>,</span>
 <span class=na>booktitle</span> <span class=p>=</span> <span class=s><span class=p>{</span>Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>month</span> <span class=p>=</span> <span class=s><span class=p>{</span>May<span class=p>}</span></span><span class=p>,</span>
 <span class=na>pages</span> <span class=p>=</span> <span class=s><span class=p>{</span>1239-1247<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines<span class=p>}</span></span><span class=p>,</span>
 <span class=na>url</span> <span class=p>=</span> <span class=s><span class=p>{</span>https://dl.acm.org/doi/10.5555/3545946.3598769<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2023<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Less+Is+More:+Refining+Datasets+for+Offline+Reinforcement+Learning+with+Reward+Machines" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Less+Is+More:+Refining+Datasets+for+Offline+Reinforcement+Learning+with+Reward+Machines" target=_blank>Crossref</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=https://wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=https://wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=https://wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>