<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=7662302B2EE2DF855D33DA34FD827164><meta name=google-site-verification content=Nj_dp8eZCY7mCHa7wHJz4MXpILpTh7vii2xAujBW5Zg><title>Feng Wu (吴锋) | Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient"><meta name=citation_author content="Yixiang Wang"><meta name=citation_author content="Feng Wu"><meta name=citation_book_title content="Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)"><meta name=citation_inbook_title content="Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)"><meta name=citation_conference_title content="The 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)"><meta name=citation_conference content="The 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)"><meta name=citation_publication_date content=2020><meta name=citation_online_date content=2020><meta name=citation_date content=2020><meta name=citation_year content=2020><meta name=citation_pdf_url content=//wufeng02.github.io/doc/pdf/WWprima20.pdf><meta name=citation_abstract content="We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution.We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments."><meta name=bepress_citation_title content="Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient"><meta name=bepress_citation_author content="Yixiang Wang"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_book_title content="Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)"><meta name=bepress_citation_date content=2020><meta name=bepress_citation_online_date content=2020><meta name=bepress_citation_pdf_url content=//wufeng02.github.io/doc/pdf/WWprima20.pdf><meta name=bepress_citation_abstract_html_url content=//wufeng02.github.io/doc/htm/WWprima20.html><meta name=bepress_citation_abstract content="We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution.We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments."><meta name=eprints.source content=//wufeng02.github.io/doc/htm/WWprima20.html><meta name=eprints.title content="Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient"><meta name=eprints.creators_name content="Yixiang Wang"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.type content=conference_item><meta name=eprints.event_type content=conference><meta name=eprints.event_title content="Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)"><meta name=eprints.date content=2020><meta name=eprints.date_type content=published><meta name=eprints.document_url content=//wufeng02.github.io/doc/pdf/WWprima20.pdf><meta name=eprints.citation content="Yixiang Wang, Feng Wu. Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient. In Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA), Nagoya, Japan, Novermber 2020."><meta name=eprints.abstract content="We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution.We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=//wufeng02.github.io/doc/htm/WWprima20.html><meta name=DC.title content="Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient"><meta name=DC.creator content="Yixiang Wang"><meta name=DC.creator content="Feng Wu"><meta name=DC.type content="Conference or Workshop Item"><meta name=DC.relation.ispartof content="Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)"><meta name=DC.issued content=2020><meta name=DC.date content=2020><meta name=DC.format content=application/pdf><meta name=DC.identifier content=//wufeng02.github.io/doc/pdf/WWprima20.pdf><meta name=DC.description content="We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution.We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments."><meta name=twitter:site content=//wufeng02.github.io/doc/htm/WWprima20.html><meta property=twitter:title content="Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient"><meta property=twitter:description content="We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution.We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments."><meta property=og:site_name content=//wufeng02.github.io/doc/htm/WWprima20.html><meta property=og:title content="Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient"><meta property=og:url content=//wufeng02.github.io/doc/pdf/WWprima20.pdf><meta property=og:description content="We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution.We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments."><link rel=canonical href=//wufeng02.github.io/doc/htm/WWprima20.html><link rel=alternate hreflang=x-default href=//wufeng02.github.io/doc/htm/WWprima20.html><link rel=alternate hreflang=en href="//wufeng02.github.io/doc/htm/WWprima20.html?lang=en"><link rel=alternate hreflang=zh href="//wufeng02.github.io/doc/htm/WWprima20.html?lang=zh"><link rel=search type=text/html href=//wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=//wufeng02.github.io/search.xml><link rel=icon href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=//wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=//wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=//wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=//wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_book_title itemprop=name>Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)</span>, </span><span class="citation_date citation_year" itemprop=datePublished>2020</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient </h1><h3 id=author class=citation_author><span itemprop=author> Yixiang Wang</span>, <span itemprop=author>Feng Wu </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution.We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments.</p> &raquo; <a href=//wufeng02.github.io/doc/pdf/WWprima20.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 566.8KB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Yixiang Wang</span>, <span class=cite_a>Feng Wu</span>. <span class=cite_t>Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient</span>. In <span class=cite_p>Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)</span>, <span class=cite_l>Nagoya, Japan,</span> <span class=cite_m>Novermber </span><span class=cite_y>2020</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=//wufeng02.github.io/doc/htm/WWprima20.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>WWprima20</span><span class=p>,</span>
 <span class=na>address</span> <span class=p>=</span> <span class=s><span class=p>{</span>Nagoya, Japan<span class=p>}</span></span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Yixiang Wang and Feng Wu<span class=p>}</span></span><span class=p>,</span>
 <span class=na>booktitle</span> <span class=p>=</span> <span class=s><span class=p>{</span>Proceedings of the 23rd International Conference on Principles and Practice of Multi-Agent Systems (PRIMA)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>month</span> <span class=p>=</span> <span class=s><span class=p>{</span>Novermber<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Policy Adaptive Multi-Agent Deep Deterministic Policy Gradient<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2020<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Policy+Adaptive+Multi-Agent+Deep+Deterministic+Policy+Gradient" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Policy+Adaptive+Multi-Agent+Deep+Deterministic+Policy+Gradient" target=_blank>Crossref</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=//wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=//wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=//wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>