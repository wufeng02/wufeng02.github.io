<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><meta name=robots content=NOODP,NOYDIR><meta name=msvalidate.01 content=7662302B2EE2DF855D33DA34FD827164><meta name=google-site-verification content=Nj_dp8eZCY7mCHa7wHJz4MXpILpTh7vii2xAujBW5Zg><title>Feng Wu (吴锋) | Offline Meta-Reinforcement Learning with Contrastive Prediction</title><meta name=gs_meta_revision content=1.1><meta name=citation_title content="Offline Meta-Reinforcement Learning with Contrastive Prediction"><meta name=citation_author content="Xu Han"><meta name=citation_author content="Feng Wu"><meta name=citation_journal_title content="Journal of Frontiers of Computer Science and Technology"><meta name=citation_journal_abbrev content=JFCST><meta name=citation_volume content=17><meta name=citation_issue content=8><meta name=citation_firstpage content=1917><meta name=citation_lastpage content=1927><meta name=citation_publication_date content=2023><meta name=citation_online_date content=2023><meta name=citation_date content=2023><meta name=citation_year content=2023><meta name=citation_doi content=10.3778/j.issn.1673-9418.2203074><meta name=citation_pdf_url content=//wufeng02.github.io/doc/pdf/HWjfcst23.pdf><meta name=citation_abstract content="Traditional reinforcement learning algorithms require lots of online interaction with the environment for training and cannot effectively adapt to changes in the task environment, making them difficult to apply to real-world problems. Offline meta-reinforcement learning provides an effective way to quickly adapt to a new task by using replay datasets of multiple tasks for offline policy learning. Applying offline meta-reinforcement learning to complex tasks will face two challenges. Firstly, reinforcement learning algorithms overestimate the value of state-action pairs not contained in the dataset and thus select non-optimal actions, resulting in poor performance. Secondly, meta-reinforcement learning algorithms need not only to learn the policy but also to have robust and efficient task inference capabilities. To address the above problems, this paper proposes an offline meta-reinfor-cement learning algorithm based on contrastive prediction. To cope with the problem of overestimation of value functions, the proposed algorithm uses behavior cloning to encourage policy to prefer actions included in the dataset. To improve the task inference capability of meta-learning, the proposed algorithm uses recurrent neural networks for task inference on the contextual trajectories of the agents and uses contrastive learning and prediction networks to analyze and distinguish potential structures in different task trajectories. Experimental results show that the agents trained by the proposed algorithm can score more than 25 percentage points when faced with unseen tasks, and it has higher meta-training efficiency and better generalization performance compared with existing methods."><meta name=bepress_citation_title content="Offline Meta-Reinforcement Learning with Contrastive Prediction"><meta name=bepress_citation_author content="Xu Han"><meta name=bepress_citation_author content="Feng Wu"><meta name=bepress_citation_journal_title content="Journal of Frontiers of Computer Science and Technology (JFCST)"><meta name=bepress_citation_volume content=17><meta name=bepress_citation_issue content=8><meta name=bepress_citation_firstpage content=1917><meta name=bepress_citation_lastpage content=1927><meta name=bepress_citation_date content=2023><meta name=bepress_citation_online_date content=2023><meta name=bepress_citation_doi content=10.3778/j.issn.1673-9418.2203074><meta name=bepress_citation_pdf_url content=//wufeng02.github.io/doc/pdf/HWjfcst23.pdf><meta name=bepress_citation_abstract_html_url content=//wufeng02.github.io/doc/htm/HWjfcst23.html><meta name=bepress_citation_abstract content="Traditional reinforcement learning algorithms require lots of online interaction with the environment for training and cannot effectively adapt to changes in the task environment, making them difficult to apply to real-world problems. Offline meta-reinforcement learning provides an effective way to quickly adapt to a new task by using replay datasets of multiple tasks for offline policy learning. Applying offline meta-reinforcement learning to complex tasks will face two challenges. Firstly, reinforcement learning algorithms overestimate the value of state-action pairs not contained in the dataset and thus select non-optimal actions, resulting in poor performance. Secondly, meta-reinforcement learning algorithms need not only to learn the policy but also to have robust and efficient task inference capabilities. To address the above problems, this paper proposes an offline meta-reinfor-cement learning algorithm based on contrastive prediction. To cope with the problem of overestimation of value functions, the proposed algorithm uses behavior cloning to encourage policy to prefer actions included in the dataset. To improve the task inference capability of meta-learning, the proposed algorithm uses recurrent neural networks for task inference on the contextual trajectories of the agents and uses contrastive learning and prediction networks to analyze and distinguish potential structures in different task trajectories. Experimental results show that the agents trained by the proposed algorithm can score more than 25 percentage points when faced with unseen tasks, and it has higher meta-training efficiency and better generalization performance compared with existing methods."><meta name=eprints.source content=//wufeng02.github.io/doc/htm/HWjfcst23.html><meta name=eprints.title content="Offline Meta-Reinforcement Learning with Contrastive Prediction"><meta name=eprints.creators_name content="Xu Han"><meta name=eprints.creators_name content="Feng Wu"><meta name=eprints.type content=article><meta name=eprints.publication content="Journal of Frontiers of Computer Science and Technology (JFCST)"><meta name=eprints.volume content=17><meta name=eprints.issue content=8><meta name=eprints.date content=2023><meta name=eprints.date_type content=published><meta name=eprints.document_url content=//wufeng02.github.io/doc/pdf/HWjfcst23.pdf><meta name=eprints.citation content="Xu Han, Feng Wu. Offline Meta-Reinforcement Learning with Contrastive Prediction. Journal of Frontiers of Computer Science and Technology (JFCST), 178:1917-1927, 2023."><meta name=eprints.abstract content="Traditional reinforcement learning algorithms require lots of online interaction with the environment for training and cannot effectively adapt to changes in the task environment, making them difficult to apply to real-world problems. Offline meta-reinforcement learning provides an effective way to quickly adapt to a new task by using replay datasets of multiple tasks for offline policy learning. Applying offline meta-reinforcement learning to complex tasks will face two challenges. Firstly, reinforcement learning algorithms overestimate the value of state-action pairs not contained in the dataset and thus select non-optimal actions, resulting in poor performance. Secondly, meta-reinforcement learning algorithms need not only to learn the policy but also to have robust and efficient task inference capabilities. To address the above problems, this paper proposes an offline meta-reinfor-cement learning algorithm based on contrastive prediction. To cope with the problem of overestimation of value functions, the proposed algorithm uses behavior cloning to encourage policy to prefer actions included in the dataset. To improve the task inference capability of meta-learning, the proposed algorithm uses recurrent neural networks for task inference on the contextual trajectories of the agents and uses contrastive learning and prediction networks to analyze and distinguish potential structures in different task trajectories. Experimental results show that the agents trained by the proposed algorithm can score more than 25 percentage points when faced with unseen tasks, and it has higher meta-training efficiency and better generalization performance compared with existing methods."><link href=http://purl.org/dc/elements/1.1/ rel=schema.DC><meta name=DC.relation content=//wufeng02.github.io/doc/htm/HWjfcst23.html><meta name=DC.title content="Offline Meta-Reinforcement Learning with Contrastive Prediction"><meta name=DC.creator content="Xu Han"><meta name=DC.creator content="Feng Wu"><meta name=DC.type content=Article><meta name=DC.relation.ispartof content="Journal of Frontiers of Computer Science and Technology (JFCST)"><meta name=DC.citation.volume content=17><meta name=DC.citation.issue content=8><meta name=DC.citation.spage content=1917><meta name=DC.citation.epage content=1927><meta name=DC.issued content=2023><meta name=DC.date content=2023><meta name=DC.format content=application/pdf><meta name=DC.identifier content=//wufeng02.github.io/doc/pdf/HWjfcst23.pdf><meta name=DC.description content="Traditional reinforcement learning algorithms require lots of online interaction with the environment for training and cannot effectively adapt to changes in the task environment, making them difficult to apply to real-world problems. Offline meta-reinforcement learning provides an effective way to quickly adapt to a new task by using replay datasets of multiple tasks for offline policy learning. Applying offline meta-reinforcement learning to complex tasks will face two challenges. Firstly, reinforcement learning algorithms overestimate the value of state-action pairs not contained in the dataset and thus select non-optimal actions, resulting in poor performance. Secondly, meta-reinforcement learning algorithms need not only to learn the policy but also to have robust and efficient task inference capabilities. To address the above problems, this paper proposes an offline meta-reinfor-cement learning algorithm based on contrastive prediction. To cope with the problem of overestimation of value functions, the proposed algorithm uses behavior cloning to encourage policy to prefer actions included in the dataset. To improve the task inference capability of meta-learning, the proposed algorithm uses recurrent neural networks for task inference on the contextual trajectories of the agents and uses contrastive learning and prediction networks to analyze and distinguish potential structures in different task trajectories. Experimental results show that the agents trained by the proposed algorithm can score more than 25 percentage points when faced with unseen tasks, and it has higher meta-training efficiency and better generalization performance compared with existing methods."><meta name=twitter:site content=//wufeng02.github.io/doc/htm/HWjfcst23.html><meta property=twitter:title content="Offline Meta-Reinforcement Learning with Contrastive Prediction"><meta property=twitter:description content="Traditional reinforcement learning algorithms require lots of online interaction with the environment for training and cannot effectively adapt to changes in the task environment, making them difficult to apply to real-world problems. Offline meta-reinforcement learning provides an effective way to quickly adapt to a new task by using replay datasets of multiple tasks for offline policy learning. Applying offline meta-reinforcement learning to complex tasks will face two challenges. Firstly, reinforcement learning algorithms overestimate the value of state-action pairs not contained in the dataset and thus select non-optimal actions, resulting in poor performance. Secondly, meta-reinforcement learning algorithms need not only to learn the policy but also to have robust and efficient task inference capabilities. To address the above problems, this paper proposes an offline meta-reinfor-cement learning algorithm based on contrastive prediction. To cope with the problem of overestimation of value functions, the proposed algorithm uses behavior cloning to encourage policy to prefer actions included in the dataset. To improve the task inference capability of meta-learning, the proposed algorithm uses recurrent neural networks for task inference on the contextual trajectories of the agents and uses contrastive learning and prediction networks to analyze and distinguish potential structures in different task trajectories. Experimental results show that the agents trained by the proposed algorithm can score more than 25 percentage points when faced with unseen tasks, and it has higher meta-training efficiency and better generalization performance compared with existing methods."><meta property=og:site_name content=//wufeng02.github.io/doc/htm/HWjfcst23.html><meta property=og:title content="Offline Meta-Reinforcement Learning with Contrastive Prediction"><meta property=og:url content=//wufeng02.github.io/doc/pdf/HWjfcst23.pdf><meta property=og:description content="Traditional reinforcement learning algorithms require lots of online interaction with the environment for training and cannot effectively adapt to changes in the task environment, making them difficult to apply to real-world problems. Offline meta-reinforcement learning provides an effective way to quickly adapt to a new task by using replay datasets of multiple tasks for offline policy learning. Applying offline meta-reinforcement learning to complex tasks will face two challenges. Firstly, reinforcement learning algorithms overestimate the value of state-action pairs not contained in the dataset and thus select non-optimal actions, resulting in poor performance. Secondly, meta-reinforcement learning algorithms need not only to learn the policy but also to have robust and efficient task inference capabilities. To address the above problems, this paper proposes an offline meta-reinfor-cement learning algorithm based on contrastive prediction. To cope with the problem of overestimation of value functions, the proposed algorithm uses behavior cloning to encourage policy to prefer actions included in the dataset. To improve the task inference capability of meta-learning, the proposed algorithm uses recurrent neural networks for task inference on the contextual trajectories of the agents and uses contrastive learning and prediction networks to analyze and distinguish potential structures in different task trajectories. Experimental results show that the agents trained by the proposed algorithm can score more than 25 percentage points when faced with unseen tasks, and it has higher meta-training efficiency and better generalization performance compared with existing methods."><link rel=canonical href=//wufeng02.github.io/doc/htm/HWjfcst23.html><link rel=alternate hreflang=x-default href=//wufeng02.github.io/doc/htm/HWjfcst23.html><link rel=alternate hreflang=en href="//wufeng02.github.io/doc/htm/HWjfcst23.html?lang=en"><link rel=alternate hreflang=zh href="//wufeng02.github.io/doc/htm/HWjfcst23.html?lang=zh"><link rel=search type=text/html href=//wufeng02.github.io/search.html><link rel=search type=application/opensearchdescription+xml href=//wufeng02.github.io/search.xml><link rel=icon href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel="shortcut icon" href=//wufeng02.github.io/media/favicon.ico type=image/x-icon><link rel=stylesheet href=//wufeng02.github.io/static/bootstrap/css/bootstrap.min.css type=text/css><script src=//wufeng02.github.io/static/jquery/jquery.min.js type=text/javascript></script><script src=//wufeng02.github.io/static/bootstrap/js/bootstrap.min.js type=text/javascript></script><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/base.min.css type=text/css><link rel=stylesheet href=//wufeng02.github.io/static/pages/css/paper.css type=text/css><!--[if lt IE 9]>
      	<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      	<script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    	<![endif]--></head><body><br><div class=container itemscope itemtype=http://schema.org/ScholarlyArticle><ol class=breadcrumb><li style="width: auto; vertical-align: top;"><a href=//wufeng02.github.io/publication.html><span itemprop=about>Publication</span></a></li><li class=active style="width: 90%;"><span itemprop=isPartOf itemscope itemtype=http://schema.org/Periodical><span class=citation_journal_title itemprop=name>Journal of Frontiers of Computer Science and Technology (JFCST)</span>, </span><span itemprop=isPartOf itemscope itemtype=http://schema.org/PublicationVolume> Volume <span class=citation_volume itemprop=volumeNumber>17</span>, </span><span itemprop=isPartOf itemscope itemtype=http://schema.org/PublicationIssue> Number <span class=citation_issue itemprop=issueNumber>8</span>, </span> Page <span itemprop=pagination>1917-1927</span>, <span class="citation_date citation_year" itemprop=datePublished>2023</span>. </li></ol><h1 id=title class=citation_title itemprop="name headline"> Offline Meta-Reinforcement Learning with Contrastive Prediction </h1><h3 id=author class=citation_author><span itemprop=author> Xu Han</span>, <span itemprop=author>Feng Wu </span></h3><h4 class="page-header clickable" data-toggle=collapse data-target=#abstract>Abstract</h4><p id=abstract class="citation_abstract collapse in" itemprop=description>Traditional reinforcement learning algorithms require lots of online interaction with the environment for training and cannot effectively adapt to changes in the task environment, making them difficult to apply to real-world problems. Offline meta-reinforcement learning provides an effective way to quickly adapt to a new task by using replay datasets of multiple tasks for offline policy learning. Applying offline meta-reinforcement learning to complex tasks will face two challenges. Firstly, reinforcement learning algorithms overestimate the value of state-action pairs not contained in the dataset and thus select non-optimal actions, resulting in poor performance. Secondly, meta-reinforcement learning algorithms need not only to learn the policy but also to have robust and efficient task inference capabilities. To address the above problems, this paper proposes an offline meta-reinfor-cement learning algorithm based on contrastive prediction. To cope with the problem of overestimation of value functions, the proposed algorithm uses behavior cloning to encourage policy to prefer actions included in the dataset. To improve the task inference capability of meta-learning, the proposed algorithm uses recurrent neural networks for task inference on the contextual trajectories of the agents and uses contrastive learning and prediction networks to analyze and distinguish potential structures in different task trajectories. Experimental results show that the agents trained by the proposed algorithm can score more than 25 percentage points when faced with unseen tasks, and it has higher meta-training efficiency and better generalization performance compared with existing methods.</p> &raquo; <a href=//wufeng02.github.io/doc/pdf/HWjfcst23.pdf class=citation_pdf_url itemprop="sameAs image" data-toggle=tooltip data-placement=right title="File Type: PDF, File Size: 839.8KB"> Read on </a><h4 class="page-header clickable" data-toggle=collapse data-target=#citation>Citation</h4><div id=citation class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=short class="btn btn-default btn-sm"><i class="glyphicon glyphicon-compressed"></i> Convert to short form </button></div><span><span class=cite_a>Xu Han</span>, <span class=cite_a>Feng Wu</span>. <span class=cite_t>Offline Meta-Reinforcement Learning with Contrastive Prediction</span>. <span class=cite_p>Journal of Frontiers of Computer Science and Technology (JFCST)</span>, <span class=cite_v>17</span>(<span class=cite_n>8</span>):<span class=cite_s>1917-1927</span>, <span class=cite_y>2023</span>.</span></div><h4 class="page-header clickable" data-toggle=collapse data-target=#bibtex>BibTex</h4><div id=bibtex class="collapse in"><div style="text-align: right; margin-bottom: 10px"><button id=copy class="btn btn-default btn-sm"><i class="glyphicon glyphicon-list-alt"></i> Copy to clipboard </button><a id=save class="btn btn-default btn-sm" href=//wufeng02.github.io/doc/htm/HWjfcst23.bib target=_blank><i class="glyphicon glyphicon-download-alt"></i> Save as file </a></div><div class=highlight><pre><span></span><span class=nc>@article</span><span class=p>{</span><span class=nl>HWjfcst23</span><span class=p>,</span>
 <span class=na>author</span> <span class=p>=</span> <span class=s><span class=p>{</span>Xu Han and Feng Wu<span class=p>}</span></span><span class=p>,</span>
 <span class=na>doi</span> <span class=p>=</span> <span class=s><span class=p>{</span>10.3778/j.issn.1673-9418.2203074<span class=p>}</span></span><span class=p>,</span>
 <span class=na>journal</span> <span class=p>=</span> <span class=s><span class=p>{</span>Journal of Frontiers of Computer Science and Technology (JFCST)<span class=p>}</span></span><span class=p>,</span>
 <span class=na>number</span> <span class=p>=</span> <span class=s><span class=p>{</span>8<span class=p>}</span></span><span class=p>,</span>
 <span class=na>pages</span> <span class=p>=</span> <span class=s><span class=p>{</span>1917-1927<span class=p>}</span></span><span class=p>,</span>
 <span class=na>title</span> <span class=p>=</span> <span class=s><span class=p>{</span>Offline Meta-Reinforcement Learning with Contrastive Prediction<span class=p>}</span></span><span class=p>,</span>
 <span class=na>volume</span> <span class=p>=</span> <span class=s><span class=p>{</span>17<span class=p>}</span></span><span class=p>,</span>
 <span class=na>year</span> <span class=p>=</span> <span class=s><span class=p>{</span>2023<span class=p>}</span></span>
<span class=p>}</span>
</pre></div></div><h4 class="page-header clickable" data-toggle=collapse data-target=#links>External Links</h4><div id=links class="collapse in"><ul><li><a href="https://scholar.google.com/scholar?q=Offline+Meta-Reinforcement+Learning+with+Contrastive+Prediction" target=_blank>Google Scholar</a></li><li><a href="https://search.crossref.org/?q=Offline+Meta-Reinforcement+Learning+with+Contrastive+Prediction" target=_blank>Crossref</a> &mdash; DOI: <a href=https://doi.org/10.3778/j.issn.1673-9418.2203074 target=_blank class=citation_doi>10.3778/j.issn.1673-9418.2203074</a></li><li><a href=https://www.engineeringvillage.com/search/quick.url target=_blank>Engineering Village</a></li><li><a href=http://www.webofknowledge.com/wos target=_blank>Web of Science</a></li></ul></div><br></div><footer class=text-center><script type=text/javascript src=//wufeng02.github.io/static/pages/js/email.js></script><noscript><img src=//wufeng02.github.io/img/email.png alt=Email></noscript></footer><br><script type=text/javascript src=//wufeng02.github.io/static/pages/js/paper.min.js></script></body></html>